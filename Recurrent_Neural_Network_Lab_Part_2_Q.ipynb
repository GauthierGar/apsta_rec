{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McDZ-FOQHtz_"
   },
   "source": [
    "Course Accreditations: Inspired from Advanced Data Science course from harvard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZzT54YUItfZ"
   },
   "source": [
    "# Course: Apprentissage statistique\n",
    "## DataSIM\n",
    "---\n",
    "## Enseignants:\n",
    " * Dawood ALCHANTI\n",
    " * Mathieu LAGRANGE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Part 2: \n",
    "### Complexity measure, behaviour analysis and usability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Materials\n",
    "#### 1. In this lab we will consider using google colab: https://colab.research.google.com/notebooks/intro.ipynb#recent=true\n",
    "\n",
    "#### 2. Press the link above, go to **file** and press **upload notebook**, then chose the file ''Recurrent_Neural_Network_Lab_Part_1''\n",
    "\n",
    "#### 3. At the beggining of your code, go to Edit, Notebook Setting, and change the configuration to GPU, so  you can use google colab gpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKvtb6Stq65P"
   },
   "source": [
    "## In this lab we will look mainly at:\n",
    "1. Recurrent Neural Networks (RNNs),\n",
    "2. LSTMs and their building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paxEtzguq-AE"
   },
   "source": [
    "## Goals: By the end of this lab, you should:\n",
    "1. be able to **use RNNs and its variants (GRU, LSTM)** using keras library based on tensorflow.\n",
    "2. **understand how any sequential data would fit** into and benefit from a recurrent architecture.\n",
    "3. become familiar with **text preprocessing and dynamic embeddings**.\n",
    "4. **understand the underlying complexity when using RNN**.\n",
    "4. **understand the gradient issues** on RNNs processing when it is deployed to process longer sentence lengths.\n",
    "5. become familiar with **different kinds of LSTM architectures**, classifiers, and sequence to sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItgNwIzBogrL"
   },
   "source": [
    "**Keywords: RNN, LSTM, RNN+CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Context we will be working on today is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MogtWRHHsKch"
   },
   "source": [
    "## **Sentiment classification on a movie review dataset using IMDb Reviews**\n",
    "\n",
    "Inspired from Kaggel competitions, more information is availabel in the following link: https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews\n",
    "\n",
    "\n",
    "**We are going to build a**:\n",
    "\n",
    "1. Dense Feedforward Neural Network without and with Embedding Layer,\n",
    "2. CNN Network, \n",
    "3. RNN Network,  \n",
    "4. LSTM Network\n",
    "\n",
    "and combine one or more of them to understand performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcSwtQIysF0J"
   },
   "source": [
    "**Understanding the Context**:\n",
    "\n",
    "1. A sentence can be thought of as a sequence of words that collectively represent meaning.\n",
    "2. Individual words impact the meaning.\n",
    "3. Thus, the context matters; words that occur earlier in the sentence influence the sentence's structure and meaning in the latter part of the sentence (e.g., Jose asked Anqi if she were going to the library today).\n",
    "4. Likewise, words that occur later in a sentence can affect the meaning of earlier words (e.g., Apple is an interesting company). \n",
    "5. If we wish to make use of a full sentence's context in both directions, then we should use a bi-directional RNN (e.g., Bi-LSTM). \n",
    "\n",
    "* For the purpose of this tutorial, we are going to restrict ourselves to only uni-directional RNNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFK2ha9Qkujh"
   },
   "outputs": [],
   "source": [
    "# Dependencies, we will rely for simplicity on Keras with backend Tensorflow\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11LKjAn4uOQ_"
   },
   "source": [
    "# **Natural Language Processing:**\n",
    "\n",
    "1. Permit Computers to have an understandable numerical representation for words.\n",
    "2. The first crucial step is to clean (pre-process) your data so that you can soundly make use of it. \n",
    "3.  Within NLP, this first step is called Tokenization and it concerns how to represent each token (a.k.a. word) of your corpus (i.e., dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac89guBXvYEF"
   },
   "source": [
    "# Data Preprocessing in NLP: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TOKENIZATION --> Word-Based Encodings and Transofrming Text to Sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwctSdjivfER"
   },
   "source": [
    "1. A **token** refers to a single, atomic unit of meaning (i.e., a **word**).\n",
    "\n",
    "2. ***How should our computers represent each word?*** We could read in our corpus word by word and store each word as a String (data structure). However, *Strings tend to use more computer memory* *than Integers* and can become cumbersome. \n",
    "\n",
    "3. We are better off **converting each distinct word to a distinct number (Integer).**\n",
    "\n",
    "4. As a simple example of tokenization, assume we have **five** **sentences** below as our **entire corpus**:\n",
    "* i have books \n",
    "* interesting books are useful \n",
    "* i have computers \n",
    "* computers are interesting and useful \n",
    "* books and computers are both valuable. \n",
    "* Bye Bye\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YBcyy4Rwtl_"
   },
   "source": [
    "**Now, let us create tokens for vocabulary based on frequency of occurrence. Hence, we assign the following tokens:**\n",
    "\n",
    "'books': 1, 'are': 2, 'computers': 3, 'i': 4, 'have': 5, 'interesting': 6, 'useful': 7, 'and': 8, 'bye': 9, 'both': 10, 'valuable': 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zstnKv4exGoE"
   },
   "source": [
    "***Hence, the representation of our sentences will be as follow:***\n",
    "\n",
    "* i have books --> [4, 5, 1]\n",
    "* interesting books are useful --> [6, 1, 2, 7]\n",
    "* i have computers --> [4, 5, 3]\n",
    "* computers are interesting and useful -->  [3, 2, 6, 8, 7]\n",
    "* books and computers are both valuable -->   [1, 8, 3, 2, 10, 11]\n",
    "* Bye Bye --> [9, 9]\n",
    "\n",
    "\n",
    "* For more information: https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNYCTovkx_HT"
   },
   "source": [
    "## How to do that automatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "             'i have books',\n",
    "             'interesting books are useful',\n",
    "             'i have computers',\n",
    "             'computers are interesting and useful',\n",
    "             'books and computers are both valuable',\n",
    "             'Bye Bye'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(sentences[i]+' : ' + str(sequences[i])) for i in range(len(sentences))][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfeOgtnAzHrA"
   },
   "source": [
    "# 2. Padding:\n",
    "\n",
    "1. If we were training our RNN one sentence at a time, it would be okay to have sentences of varying lengths. Therefore, we can first feed the 1st sentence 'i have books' with sequence representation [4, 5, 1], which have size 3. Then the second sentence 'interesting books are useful' [6, 1, 2, 7], which have size 4. And so on...\n",
    "\n",
    "2. However, as with any neural network, it can be sometimes advantageous to train inputs in batches. Therefore, our input tensors need to be of the same length/dimensions. That is, we can consider the maximal length of our sequence representation, **in the upper example**, the sentence 'books and computers are both valuable' with representation of [1, 8, 3, 2, 10, 11] is the longest sentence with a size of 6. Hence, we can consider the longest sentence length in our training set as our maximal length. Afterwards, each of the above sentences will be padded by zeros until the maximal length is reached. That is:\n",
    "\n",
    "---\n",
    "* i have books :[ 0,  0,  0,  4,  5,  1]\n",
    "* interesting books are useful : [ 0,  0,  6,  1,  2,  7]\n",
    "* i have computers :[ 0,  0,  0,  4,  5,  3]\n",
    "* computers are interesting and useful : [ 0,  3,  2,  6,  8,  7]\n",
    "* books and computers are both valuable : [1, 8, 3, 2, 10, 11]\n",
    "* Bye Bye :  [ 0,  0,  0,  0,  9,  9]\n",
    "\n",
    "\n",
    "\n",
    "For more information: https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do that automatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PaddedSequence = sequence.pad_sequences(sequences, maxlen=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(sentences[i]+' : ' + str(PaddedSequence[i])) for i in range(len(sentences))][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wJvYDGJw1FP"
   },
   "source": [
    "## Thankfully, our dataset is already represented in such a tokenized form and no further preprocessing is required. However, it is not padded and we should perform this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let us **Load the Dataset from imdb library** using load_data fuction.\n",
    "\n",
    "* We will **strict our vocabulary size** to 10000 to have a finite vocabulary to make sure that our word matrices are not arbitrary small, therefore we will set vocabulary_size = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small\n",
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "print('Number of training reviews', len(X_train))\n",
    "print('Length of first and fifth review before padding', len(X_train[0]) ,len(X_train[4]))\n",
    "print('First review vector', X_train[0])\n",
    "print('First review label', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, let us now pad our sentences.\n",
    "\n",
    "* We will also want to **have a finite length of reviews** and not to have to process really long sentences, therefore we will set *max_review_length = 500*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* **Hint**: use the sequence library and then use pad_sequences function. Here we \n",
    "need to give as an argument our data and the maximum length to pad with.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.1 Pad the train and the test dataset with a finite length of reviews of length 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = xxx # replace xxx by your code\n",
    "\n",
    "X_train = sequence.pad_sequences(xxx, maxlen=xxx) # replace xxx by your code\n",
    "X_test = sequence.pad_sequences(xxx, maxlen=xxx)  # replace xxx by your code\n",
    "print('Length of first and fifth review after padding', len(X_train[0]) ,len(X_train[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dblcl8X_lkSD"
   },
   "source": [
    "# Q.2 MODEL 1A : FEED-FORWARD NETWORKS WITHOUT EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NE-xbN3k0n64"
   },
   "source": [
    "1. Build a single-layer feed-forward net with a hidden layer of 250 nodes to do classification. \n",
    "2. Each input must be a 500-dim vector of tokens since we padded all our sequences to size 500.\n",
    "3. Calculate the number of parameters involved in this network.\n",
    "\n",
    "---\n",
    "Remark: Check on \n",
    "* https://keras.io/guides/functional_api/ \n",
    "* https://keras.io/guides/sequential_model/\n",
    "\n",
    "to build your 1st model\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTb7rGOXlrX4"
   },
   "source": [
    "**Here you must**: \n",
    "1. Define your model.\n",
    "2. add Dense layers.\n",
    "3. compile your loss\n",
    "4. print your model summary and check on the total number of model parameters.\n",
    "5. evaluate your model over the test set.\n",
    "6. print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydmq25pClgqa",
    "outputId": "b7479323-f58f-4aae-88d1-b9829502616e"
   },
   "outputs": [],
   "source": [
    "#1. Build a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# define single layer feed-forward net with 250 nodes with relu activation, use Dense function.\n",
    "model.add(xxx) # replace xxx by your code.\n",
    "\n",
    "# define a single Dense layer with sigmoid activation that perform sentiment classification, here we will have\n",
    "# our output either 0 or 1, therefor the number of output nodes is?\n",
    "model.add(xxx) # replace xxx by your code.\n",
    "\n",
    "# Use binary_crossentropy as loss function, use adam as optimizer and measure the accuracy\n",
    "model.compile(xxx) # replace xxx by your code.\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model over the training set and consdider the test set as your validation.\n",
    "# Here, we will use 10 epochs, with batch of 128\n",
    "model.fit(xxx, epochs=10, batch_size=128, verbose=2) # replace xxx by your code.\n",
    "\n",
    "# Final evaluation of the model: Get the model scores over the test set\n",
    "scores = model.evaluate(xxx, xxx, verbose=0)  # replace xxx by your code.\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yk_Qhx8Nl0xK"
   },
   "source": [
    "# *Discussion*: \n",
    "#### 1. Comment on the model performance (analyse the loss and the accuracy over the test set)? \n",
    "#### 2. What was wrong with tokenization? Do you think it is a representative way to consider it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMcyKYHHl3w9"
   },
   "source": [
    "# Q.3  MODEL 1B : FEED-FORWARD NETWORKS WITH EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7WMTEE32UTW"
   },
   "source": [
    "---\n",
    "**What is an embedding layer ?**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYQRLZMB2YVC"
   },
   "source": [
    "An embedding is a \"**distributed representation**\" (e.g., vector) of a particular atomic item (e.g., word token, object, etc). \n",
    "\n",
    "When representing items by embeddings:\n",
    "\n",
    "* each distinct item should be represented by its own unique embedding\n",
    "* the semantic similarity between items should correspond to the similarity between their respective embeddings (i.e., *words that are more similar to one another should have embeddings that are more similar to each other*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVNTtGUx28Fe"
   },
   "source": [
    "*In general, though, one can view the embedding process as a linear projection from one vector space to another (e.g., a vector space of unique words being mapped to a world of fixed-length, dense vectors filled with continuous-valued numbers.*\n",
    "\n",
    "---\n",
    "**For NLP:** \n",
    "1. we usually use embeddings to project the one-hot encodings of words on to a lower-dimensional continuous space (e.g., vectors of size 100) so that the input surface is dense and possibly smooth. \n",
    "2. Thus, one can view this embedding layer process as just a transformation from $\\mathbb{R}^{input}$ to $\\mathbb{R}^{emb}$.\n",
    "---\n",
    "\n",
    "\n",
    "**One hot Encoding:** a vector that is the length of the entire vocabulary, and it is filled with all zeros except for a single value of 1 that corresponds to the particular word.\n",
    "\n",
    "\n",
    "\n",
    "Check on the following source to know how to use the Embedding layer:\n",
    "https://keras.io/api/layers/core_layers/embedding/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L1wjLa3lwrH"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrtjmf6ml9vJ",
    "outputId": "e3b20125-4b36-4153-d1f8-1562501e363c"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# inputs will be converted from batch_size * sentence_length to batch_size*sentence_length*embedding _dim\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Now repeat your previous defined model\n",
    "# -------------------------------------\n",
    "\n",
    "# define single layer feed-forward net with 250 nodes with relu activation, use Dense function.\n",
    "model.add(xxx) # replace xxx by your code.\n",
    "\n",
    "# define a single Dense layer with sigmoid activation that perform sentiment classification, here we will have\n",
    "# our output either 0 or 1, therefor the number of output nodes is?\n",
    "model.add(xxx) # replace xxx by your code.\n",
    "\n",
    "# Use binary_crossentropy as loss function, use adam as optimizer and measure the accuracy\n",
    "model.compile(xxx) # replace xxx by your code.\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrGclhtn5iTC"
   },
   "source": [
    "# Train the defined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyQ9WTmxl_YF",
    "outputId": "a81ef41c-6e15-4234-c961-4105c35c77d6"
   },
   "outputs": [],
   "source": [
    "# Train the model over the training set and consdider the test set as your validation.\n",
    "# Here, we will use 10 epochs, with batch of 128\n",
    "model.fit(xxx, epochs=10, batch_size=128, verbose=2) # replace xxx by your code.\n",
    "\n",
    "# Final evaluation of the model: Get the model scores over the test set\n",
    "scores = model.evaluate(xxx, xxx, verbose=0)  # replace xxx by your code.\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atsMgSXO5lrj"
   },
   "source": [
    "# *Discussion*:  \n",
    "#### 1. Compare the performance with and without Embedding.\n",
    "#### 2. What do you conclude?\n",
    "#### 3. What is the main advantages of using Embedding layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab0BizGImtZp"
   },
   "source": [
    "# Q.4 MODEL 2 : Build a CNN based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACsyuIEQ56Uz"
   },
   "source": [
    "1. Text can be thought of as **1-dimensional sequence** (a single, long vector) \n",
    "2. Therefore, we can apply **1D Convolutions** over a set of word embeddings. \n",
    "\n",
    "\n",
    "* Use the model developed in Model 1B, modify it to include 1D Conv layer with 32 filters and kernel size of 3 followed by max pooling operation with stride by 2 for downsampling the vector size.\n",
    "* feed-forward layer of 250 nodes, and ReLU and Sigmoid activations as appropriate.\n",
    "* Fit the model over the training set\n",
    "* Evaluate the model performace over the test set.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h9np-Ni8Mg2"
   },
   "source": [
    "\n",
    "**More resources on:** **Understanding Convolutions in Text can be found in:**\n",
    "\n",
    "http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/11/27/Understanding-Convolutions-In-Text/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J5jomN5hmHhZ",
    "outputId": "e8352b0d-a14c-40d6-9fd2-de97deb6834e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create the CNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "\n",
    "# Define the parameters of the connv1D with 32 filters and kernal size of 3x3.\n",
    "model.add(Conv1D(xxxx, padding='same', activation='relu')) # replace xxx by your code.\n",
    "\n",
    "# perform downsampling by 2 using maxpooling\n",
    "model.add(MaxPooling1D(xxx)) # replace xxx by your code.\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# Now repeat your previous defined model\n",
    "# -------------------------------------\n",
    "\n",
    "# define single layer feed-forward net with 250 nodes with relu activation, use Dense function.\n",
    "model.add(xxx) # replace xxx by your code.\n",
    "\n",
    "# define a single Dense layer with sigmoid activation that perform sentiment classification, here we will have\n",
    "# our output either 0 or 1, therefor the number of output nodes is?\n",
    "model.add(xxx) # replace xxx by your code.\n",
    "\n",
    "# Use binary_crossentropy as loss function, use adam as optimizer and measure the accuracy\n",
    "model.compile(xxx) # replace xxx by your code.\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWXZLcQg7wey"
   },
   "source": [
    "# Discusion: \n",
    "### comment on the model performance in term of:\n",
    "1. accuracy, \n",
    "2. running tim and \n",
    "3. complexity: consider using total number of parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98apK3iam4if"
   },
   "source": [
    "## Q.5 MODEL 3 : Simple Recurrent Neural Network RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0SxFNX_m8yl"
   },
   "source": [
    "More resources on understanding RNN and LSTM are:\n",
    "\n",
    "1. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "2. http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tA91vwSj8oiP"
   },
   "source": [
    "* An RNN is similar to a feed-forward neural network in that there is an input layer, a hidden layer, and an output layer. \n",
    "* The input layer is fully connected to the hidden layer, and the hidden layer is fully connected to the output layer.\n",
    "*  However, the hidden layer for a given time  $t$ is not only based on the input layer at time  $t$ but also the hidden layer from time $t-1$.\n",
    "* Mathematically, a simpleRNN can be defined by the following recurrence relation:\n",
    "\n",
    "\n",
    "---\n",
    "$$h_{t} = \\sigma(W[h_{t-1},x_{t}]+b)$$\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVDt_GHMmwKg",
    "outputId": "f7a90a47-2f92-4125-97fa-16d6ebbe1800"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model.add(Embedding(xxx)) # Replace your code by xxx\n",
    "\n",
    "# Add a recurrent layer using SimpleRNN with 100 hidden units\n",
    "model.add(xxx) # Replace your code by xxx\n",
    "\n",
    "# Add a Dense layer for computing the output scores with Sigmoid activations\n",
    "model.add(xxx) # Replace your code by xxx\n",
    "\n",
    "# Compile the model by defining the loss, the optimizer and the metrics, similar to what you have done before\n",
    "model.compile(xxx) # Replace your code by xxx\n",
    "print(model.summary())\n",
    "\n",
    "# Train the defined model over the training set\n",
    "model.fit(xxx, epochs=10, batch_size=128, verbose=2) \n",
    "\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KHeWx-z-UET"
   },
   "source": [
    "# **Discussion**: \n",
    "1. Comment on the results, any drop or gain in the performance? \n",
    "2. What went wrong?\n",
    "3. Compare the model complexity with CNN model and with Dense Model with Embedding.\n",
    "4. Can you think of any trick to decrease the model complexity while still using RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remark: as we have seen in the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XRw8R12-Q_n"
   },
   "source": [
    "## The main problem when using RNN is the vanishing/exploding of the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-GA8uYy-h1C"
   },
   "source": [
    "Let us use sigmoid activations as example. Derivative of a sigmoid can be written as:\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x).\\sigma(1-x)$$\n",
    "\n",
    "* Remember that an RNN is a very deep feed-forward network when unrolled in time!\n",
    "* Hence, backpropagation happens from $h_{t}$ all the way to $h_{1}$.\n",
    "*  Also, sigmoid gradients are multiplicatively dependent on the value of sigmoid.\n",
    "*  Hence, if the non-activated output of any layer $h_{l}$ is $<0$, then $\\sigma$ tends to 0, leading to **the gradient vanishing problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35D6yd4F_lrW"
   },
   "source": [
    "# LSTM: Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB3k44yk_oUZ"
   },
   "source": [
    "1. LSTM and GRU are two sophisticated implementations of RNNs that have gates (one could say that their success hinges on using gates). \n",
    "2. A gate emits probability between 0 and 1. For instance, LSTM is built on these state updates:\n",
    "\n",
    "\n",
    "Let us assume that $L$ is a linear transformation $L(x) = W*x + b$\n",
    "\n",
    "* Forget Gate: $f_t = \\sigma(L[h_{t-1},x_t])$\n",
    "* Input Gate: $i_t = \\sigma(L[h_{t-1},x_t])$\n",
    "* Output Gate: $o_t = \\sigma(L[h_{t-1},x_t])$\n",
    "* Cell State: $\\hat c_t = \\tanh(L[h_{t-1},x_t])$\n",
    "\n",
    "Now, using the forget gate, the neural network can learn to **control how much information it has to retain or forget**:\n",
    "\n",
    "* $c_t = f_t*c_{t-1} + i_t*\\hat c_t$\n",
    "\n",
    "Thus the** hidden state update** is:\n",
    "\n",
    "* $o_t = o_t*\\tanh(c_{t})$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duKLxAghnaLg"
   },
   "source": [
    "# Q.6 MODEL 4 : Building a LSTM based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCjArCraBYeY"
   },
   "source": [
    "* Now, let's use an LSTM model to do classification! \n",
    "\n",
    "1. To make it a fair comparison to the SimpleRNN, let's start with the same architecture hyper-parameters (e.g., number of hidden nodes, epochs, and batch size). \n",
    "\n",
    "2. Then, experiment with increasing the number of nodes, stacking multiple layers\n",
    "\n",
    "3. Check the number of parameters that this model entails.\n",
    "\n",
    "\n",
    "\n",
    "More information are available at: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Q-7saSMnSY6",
    "outputId": "eebfdfe1-c956-4a30-bac4-f42da41a3ae5"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model.add(xxx)\n",
    "\n",
    "# Add a recurrent layer using LSTM with 100 hidden units and perform dropout with probability of 0.3\n",
    "model.add(xxx)\n",
    "\n",
    "# Add a Dense layer for computing the output scores with Sigmoid activations\n",
    "model.add(xxx)\n",
    "\n",
    "# Compile the model by defining the loss, the optimizer and the metrics, similar to what you have done before\n",
    "model.compile(xxx)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the defined model over the training set\n",
    "model.fit(xxx) # Replace your code by xxx\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSlrHmkJnkdZ"
   },
   "source": [
    "# Q.7 MODEL 5 : Combining both the CNN with LSTM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5xfdTR-Bv80"
   },
   "source": [
    "1. CNNs are good at learning spatial features, and sentences can be thought of as 1-D spatial vectors (dimensionality is determined by the number of words in the sentence). \n",
    "\n",
    "2. Here we want to apply an LSTM over the features learned by the CNN (after a maxpooling layer).\n",
    "\n",
    "\n",
    "3. By doing that, we can leverages the power of CNNs and LSTMs combined! \n",
    "\n",
    "4. We expect the CNN to be able to pick out invariant features across the 1-D spatial structure (i.e., sentence) that characterize good and bad sentiment.\n",
    "\n",
    "5. This learned spatial features may then be learned as sequences by an LSTM layer, and the final classification can be made via a feed-forward connection to a single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42YFjeJDoG-A",
    "outputId": "11bec820-88af-4068-e0eb-44505a08c86d"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model.add(xxxx) # Replace your code by xxx\n",
    "\n",
    "# Define a Conv1D layer with 32 filters and 3x3 kernel size and relue activation\n",
    "model.add(Conv1D(xxx)) # Replace your code by xxx\n",
    "\n",
    "# Perform maxpooling for downsampling the previous layer by 2\n",
    "model.add(MaxPooling1D(pool_size=xxx)) # Replace your code by xxx\n",
    "\n",
    "# Add a recurrent layer using LSTM with 100 hidden units and perform dropout with probability of 0.3\n",
    "model.add(xxx) # Replace your code by xxx\n",
    "\n",
    "# Add a Dense layer for computing the output scores with Sigmoid activations\n",
    "model.add(xxx) # Replace your code by xxx\n",
    "\n",
    "# Compile the model by defining the loss, the optimizer and the metrics, similar to what you have done before\n",
    "model.compile(xxx) # Replace your code by xxx\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the defined model over the training set\n",
    "model.fit(xxx) # Replace your code by xxx\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx4HSSrBtIc7"
   },
   "source": [
    "# Discusion: \n",
    "1. Compar ethe model performance to RNN, and to CNN based model.\n",
    "2. Elaborate on the model complexity in comparison to RNN and to CNN model\n",
    "3. complexity: consider using total number of parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LdQbdwWCOTw"
   },
   "source": [
    "# General Discusion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IotH1dqPCmyD"
   },
   "source": [
    "* **Please draw out what do you conclude regarding**:\n",
    "\n",
    "1. What is the benefits of embedding layer? and how it contribute to improve the model performance?\n",
    "2. What do you expect after adding a CNN layer? what type of possible features are extracted? \n",
    "3. Does leveraging Temporal information using RNN and its variant help?\n",
    "4. Why RNN may fail on long sentences?\n",
    "5. What is in your opinion the limitations/advantages of CNN, RNN or LSTM?\n",
    "\n",
    "\n",
    "* **You can base your discussion on the basis of:**\n",
    "1. performance, \n",
    "2. memory usage and model complexity\n",
    "3. leveraging the temporally connected information contained in the inputs\n",
    "4. performance vs memory benefits of CNNs vs RNNs \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1kEAaaHDxJW"
   },
   "source": [
    "# **Bonus**: Import GRU and check the performance against LSTM in terms of accuracy and model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J75JL3vnCjbi"
   },
   "outputs": [],
   "source": [
    "from keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAfZzQM5Drp2",
    "outputId": "e38a11b3-81f7-4199-e4c1-4f853bc50622"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(GRU(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model.add(xxxx) # Replace your code by xxx\n",
    "\n",
    "# Define a Conv1D layer with 32 filters and 3x3 kernel size and relue activation\n",
    "model.add(Conv1D(xxx)) # Replace your code by xxx\n",
    "\n",
    "# Perform maxpooling for downsampling the previous layer by 2\n",
    "model.add(MaxPooling1D(pool_size=xxx)) # Replace your code by xxx\n",
    "\n",
    "# Add a recurrent layer using GRU with 100 hidden\n",
    "model.add(xxx) # Replace your code by xxx\n",
    "\n",
    "# Add a Dense layer for computing the output scores with Sigmoid activations\n",
    "model.add(xxx) # Replace your code by xxx\n",
    "\n",
    "# Compile the model by defining the loss, the optimizer and the metrics, similar to what you have done before\n",
    "model.compile(xxx) # Replace your code by xxx\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the defined model over the training set\n",
    "model.fit(xxx) # Replace your code by xxx\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8mj3aw5EUvj"
   },
   "source": [
    "# Consider GRU with Dropout as a way of Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDw8XLQAEU5o",
    "outputId": "dc97a880-4abd-416f-f02f-a822ffadc7d3"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model.add(xxxx) # Replace your code by xxx\n",
    "\n",
    "# Define a Conv1D layer with 32 filters and 3x3 kernel size and relue activation\n",
    "model.add(Conv1D(xxx)) # Replace your code by xxx\n",
    "\n",
    "# Perform maxpooling for downsampling the previous layer by 2\n",
    "model.add(MaxPooling1D(pool_size=xxx)) # Replace your code by xxx\n",
    "\n",
    "# Add a recurrent layer using GRU with 100 hidden and apply dropout with probability of 0.3\n",
    "model.add(xxx) # Replace your code by xxx\n",
    "\n",
    "# Add a Dense layer for computing the output scores with Sigmoid activations\n",
    "model.add(xxx) # Replace your code by xxx\n",
    "\n",
    "# Compile the model by defining the loss, the optimizer and the metrics, similar to what you have done before\n",
    "model.compile(xxx) # Replace your code by xxx\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the defined model over the training set\n",
    "model.fit(xxx) # Replace your code by xxx\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OjNKI-6Eicb"
   },
   "source": [
    "# Discussion:\n",
    "1. Compare the model performance when using GRU with and without dropout\n",
    "2. Comment on the results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Recurrent Neural Network Lab Part 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
