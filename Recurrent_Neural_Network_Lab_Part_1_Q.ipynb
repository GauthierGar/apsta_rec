{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMu7kJGbKWZy"
   },
   "source": [
    "# Course: Apprentissage statistique\n",
    "## DataSIM\n",
    "---\n",
    "## Enseignants:\n",
    " * Dawood ALCHANTI\n",
    " * Mathieu LAGRANGE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--HTRrZGZwjg"
   },
   "source": [
    "# Recurrent Neural Network part 1: Implementation from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLfpIlW-Zwjg"
   },
   "source": [
    "## 1. Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GMqgayWZwjg"
   },
   "source": [
    "#### 1. In this lab we will consider using google colab: https://colab.research.google.com/notebooks/intro.ipynb#recent=true\n",
    "\n",
    "#### 2. Press the link above, go to **file** and press **upload notebook**, then chose the file ''Recurrent_Neural_Network_Lab_Part_1''\n",
    "\n",
    "#### 3. At the beggining of your code, go to Edit, Notebook Setting, and change the configuration to GPU, so  you can use google colab gpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XflhSOK2Kjr2"
   },
   "source": [
    "## 2. Goal: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AlPaxrxZwjh"
   },
   "source": [
    "### The objective of this lab part 1 is to:\n",
    "#### 1. Learn how to implement from scratch the RNN layer using Tensorflow framework.\n",
    "#### 2. How to use it on Images for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abCAucO8Zwjh"
   },
   "source": [
    "## 3. Implementation: We will rely on Tensorflow Version 1.x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXJFfcimJ8Wp"
   },
   "source": [
    "### Load the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSayEYieNQ74"
   },
   "outputs": [],
   "source": [
    "# The following command will ensure that we load the tensorflow version 1.15.2\n",
    "# which we will use for this lab, any other version of 1.x are also applicable.\n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDCb2JKJJu-r"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzuFrOSENGUP"
   },
   "outputs": [],
   "source": [
    "# Check if we are using the right version\n",
    "print('TensorFlow Version: ',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5OvGykWUcH2"
   },
   "outputs": [],
   "source": [
    "# an attempt to turn off all possible warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GB_xNZi1KJ15"
   },
   "source": [
    "### **MNIST Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34EbpEunP9b7"
   },
   "source": [
    "It is a dataset of handwritten digits. \n",
    "MNIST is kind of benchmark of datasets for deep learning. \n",
    "One other reason that we use the MNIST is that it is easily accesible through Tensorflow library.\n",
    "\n",
    "1. The dataset contains 55,000 examples for training, 5,000 examples for validation and 10,000 examples for testing. \n",
    "2. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28x28 image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTkTThkDToWr"
   },
   "source": [
    "#### In this section, we'll write the function which automatically loads the MNIST data and returns it in our desired shape and format. \n",
    "\n",
    "We willll simply write a function (load_MNIST_data) which has two mode: \n",
    "1. train (which loads the training and validation images and their corresponding labels)\n",
    "2. test (which loads the test images and their corresponding labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJYgj_MPTkD8"
   },
   "outputs": [],
   "source": [
    "# Please read the code to understand what is going on. No action is required.\n",
    "def load_MNIST_data(mode='train'):\n",
    "    \"\"\"\n",
    "    Function to download and load the MNIST data\n",
    "    :param mode: train or test\n",
    "    :return: images and the corresponding labels\n",
    "    \"\"\"\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "    if mode == 'train':\n",
    "        x_train, y_train, x_valid, y_valid = mnist.train.images, mnist.train.labels, \\\n",
    "                                             mnist.validation.images, mnist.validation.labels\n",
    "        return x_train, y_train, x_valid, y_valid\n",
    "    elif mode == 'test':\n",
    "        x_test, y_test = mnist.test.images, mnist.test.labels\n",
    "    return x_test, y_test\n",
    "\n",
    "def Data_Shuffeling(x, y):\n",
    "    \"\"\" Randomizes the order of data samples and their corresponding labels\"\"\"\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    shuffled_x = x[permutation, :]\n",
    "    shuffled_y = y[permutation]\n",
    "    return shuffled_x, shuffled_y\n",
    "\n",
    "def get_next_batch(x, y, start, end):\n",
    "    \"\"\" Iterates over the dataset in batches\"\"\"\n",
    "    x_batch = x[start:end]\n",
    "    y_batch = y[start:end]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIKrpLzoUsxB"
   },
   "source": [
    "### **Load the dataset using the above functions and display the vector sizes**\n",
    "Now we can use the defined helper function in \"train\" mode which loads the train and validation images and their corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8zMlTlHZwjj"
   },
   "source": [
    "### Q.1\n",
    "1. Use the function load_MNIST_data in mode train, make sure you understood and read the functions.\n",
    "2. replace xxx by your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fUt9EzDP77L"
   },
   "outputs": [],
   "source": [
    "xxx = load_MNIST_data(xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gk5LTlAkZwjj"
   },
   "source": [
    "### Q.2\n",
    "* Display the size of the training and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMO9X13wZwjj"
   },
   "outputs": [],
   "source": [
    "print(\"Size of:\")\n",
    "print('Training Set', xxx)   #replace xxx by your code \n",
    "print('Valdiation Set', xxx) #replace xxx by your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbGWJzctZwjj"
   },
   "source": [
    "### Q.3 Choose one example randomly from the training set and display its label:\n",
    "1. Check wether any of the label vectors are One Hot Encoded or not?\n",
    "2. If Yes, turn it into categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTMUtBm9P79i"
   },
   "outputs": [],
   "source": [
    "print('Vector: ', xxx)\n",
    "print('The corresponding categorical value of the one Hot Encoded Vector is : ', xxxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sCTAeNWZwjk"
   },
   "source": [
    "## Data Formate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUwElmT4TNaf"
   },
   "source": [
    "1. To classify images using a recurrent neural network, we consider every image row as a sequence of pixels.\n",
    "2. Because MNIST image shape is 28x28 pixels, we will then handle 28 sequences of 28 timesteps for every sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxN6VTLhZwjk"
   },
   "source": [
    "### Q.4 Define the following hyper-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZ5vc8wnOInF"
   },
   "outputs": [],
   "source": [
    "# Data Dimension\n",
    "num_input = xxx          # MNIST data input (image shape: 28x28)\n",
    "timesteps = xxx          # Timesteps\n",
    "n_classes = xxx          # Number of classes, one class per digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HjS8Q4EU5Fd"
   },
   "source": [
    "### Q.5  Network Hyper-parameters and configurations:\n",
    "Let us first define:\n",
    "1. the number of the hidden units for the RNN: consider 128 units for this lab, at home, you can try to change this value and check the model performance and complexity.\n",
    "2. the learning rate: consider 0.001 for this lab, at home, you can try to change this value and check the model performance.\n",
    "3. the number of training epochs: consider 10 epochs for this lab, at home, you can try to change this value and check the model performance.\n",
    "4. the batch size: consider 100 examples for this lab, at home, you can try to change this value and check the model performance and complexity.\n",
    "5. the frequency of the displaying the training results (just for visualization): consider 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOof7eV_TdTb"
   },
   "outputs": [],
   "source": [
    "num_hidden_units = xxx  # Number of hidden units of the RNN\n",
    "learning_rate = xxx # The optimization initial learning rate\n",
    "epochs = xxx           # Total number of training epochs\n",
    "batch_size = xxx      # Training batch size\n",
    "display_freq = xxx    # Frequency of displaying the training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77Um0hkaVFmB"
   },
   "source": [
    "### **Functions that initialize the weights and the biases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZILUlPKfOIpf"
   },
   "outputs": [],
   "source": [
    "# weight and bais wrappers: No action is required. Please try to understand the code before proceeding.\n",
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W',\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b',\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Rk655WLZwjl"
   },
   "source": [
    "# Recurrent Neural Network Layer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l07OcKDFVw0f"
   },
   "source": [
    "###  Implementation Remarks:\n",
    "1. The function **recurrent_layer** must take an input x, a 3D tensor, the first entry will represent a batch of rairning examples, the second tensor represent the number of time steps that each example is made of, and the last tensor represent the number of vector size of each input time steps. Remember, here we are takng an image of size hxw=28x28, therefore, here, we consider w=28 as time steps and h=28 as time step feature vector.\n",
    "2. Use the BasicRNNCell from rnn  as a helper function, look for documentation on how to use it.\n",
    "3. Use the static_rnn from rnn as a helper function, look for documentation on how to use it.\n",
    "4. Get the final output with linear activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqPILk43Zwjl"
   },
   "source": [
    "### Q.6 Implement RNN and follow the commented instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPXNZZUKOXjO"
   },
   "outputs": [],
   "source": [
    "def recurrent_layer(x, weights, biases, timesteps, num_hidden):\n",
    "    '''\n",
    "    This function must accept an input variable x, the W and b, the timesteps number, and the number of the hidden units.\n",
    "\n",
    "      # Prepare data shape to match `rnn` function requirements\n",
    "      # Current data input shape: (batch_size, timesteps, n_input)\n",
    "      # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    '''\n",
    "    # 1. Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input).\n",
    "    x = tf.unstack(xxx, timesteps, 1)\n",
    "\n",
    "    # 2. Define a rnn cell using tensorflow rnn library, use BasicRNNCell.\n",
    "    rnn_cell = rnn.BasicRNNCell(xxx)\n",
    "\n",
    "    # 3. Get lstm cell final output:\n",
    "        # If no initial_state is provided, dtype must be specified as float32\n",
    "        # If no initial cell state is provided, tf will initialize it to zero\n",
    "        \n",
    "    states_series, current_state = rnn.static_rnn(xxx, xxx, dtype=tf.float32)\n",
    "\n",
    "    # 4. Apply Linear activation, using rnn inner loop last output. \n",
    "    return tf.matmul(xxx, xxx) + xxx\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oLFR8R6Zwjm"
   },
   "source": [
    "# Q.7 Build your Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-LoybxmZwjm"
   },
   "source": [
    "1. Define a placeholder that accept the input and the label, we will use tf.placeholder. Enter the right shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xd-p2C0MOXlz"
   },
   "outputs": [],
   "source": [
    "# Placeholders for inputs (x) and outputs(y): \n",
    "x = tf.placeholder(tf.float32, shape=[None, xxx, xxx], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, xxx], name='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqRKfKRHZwjm"
   },
   "source": [
    "2. Use the functions weight_variable and bias_variable to define a set of weights [W,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhH1y0GpZwjm"
   },
   "outputs": [],
   "source": [
    "# create weight matrix initialized randomely from N~(0, 0.01)\n",
    "W = weight_variable(shape=[xxx, xxx])\n",
    "\n",
    "# create bias vector initialized as zero\n",
    "b = bias_variable(shape=[xxx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9CfelfqZwjn"
   },
   "source": [
    "3. Use [W, b] in recurrent_layer function we defined previously to get the rnn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-xs1wVbOXob"
   },
   "outputs": [],
   "source": [
    "# Get the RNN ouput\n",
    "output_logits = recurrent_layer(xxxx, xxx, xxx, xxx, xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt9QbZJ6Zwjn"
   },
   "source": [
    "4. Compute the Prediction using Softmax function, you can directly use builtin function from tf.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xcc8BZ2Zwjn"
   },
   "outputs": [],
   "source": [
    "# Compute the prediction using softmax\n",
    "y_pred = tf.nn.softmax(xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI8I4mapZwjn"
   },
   "source": [
    "5. Compute the class predictions: turn the output_logits into categorical by taking the argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nv0gJu3HZwjn"
   },
   "outputs": [],
   "source": [
    "# Perform Model predictions\n",
    "cls_prediction = tf.argmax(xxx, axis=1, name='predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4CDuqEEZwjn"
   },
   "source": [
    "6. Define the model loss function:\n",
    "    1. we will use the tf.softmax_cross_entropy_with_logits\n",
    "    2. then we will reduce the mean using  tf.reduce_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-ArjbgYZwjn"
   },
   "outputs": [],
   "source": [
    "# Define the loss function,\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=xxx, logits=xxx), name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcYKH2QbZwjo"
   },
   "source": [
    "7. Use Adam Optimizer: tf.train.AdamOptimizer over the defined loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj8V2frIZwjo"
   },
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=xxx, name='Adam-op').minimize(xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fyw-fDvZwjo"
   },
   "source": [
    "8. Compute the model performance\n",
    "    1. Compute the correct prediction \n",
    "    2. Compute the total accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-P-Z1yi0OXqs"
   },
   "outputs": [],
   "source": [
    "# Define the accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(xxx, 1), tf.argmax(xxx, 1), name='correct_pred')\n",
    "accuracy = tf.reduce_mean(tf.cast(xxx, tf.float32), name='accuracy')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad9ghPOHOXtR"
   },
   "outputs": [],
   "source": [
    "# no changes should be made here, keep it as it is.\n",
    "init = tf.global_variables_initializer() # Creating the op for initializing all variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ufd5ERazW-jn"
   },
   "source": [
    "### **Run a training Session**\n",
    "1. Train over the training database.\n",
    "2. Validate over the validation set.\n",
    "3. Print the loss, train and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZePaWQiqOXvH"
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "global_step = 0\n",
    "# Number of training iterations in each epoch\n",
    "num_tr_iter = int(len(y_train) / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    print('Training epoch: {}'.format(epoch + 1))\n",
    "    \n",
    "    \n",
    "    #1. Shuffel the dataset:\n",
    "    x_train, y_train = Data_Shuffeling(xxx, xxx)  # change xxx by your code\n",
    "    \n",
    "    for iteration in range(num_tr_iter):\n",
    "        global_step += 1\n",
    "        \n",
    "        start = iteration * batch_size\n",
    "        end = (iteration + 1) * batch_size\n",
    "        \n",
    "        x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
    "        x_batch = x_batch.reshape((batch_size, timesteps, num_input))\n",
    "        \n",
    "        \n",
    "        # 2. Run optimization op (backprop)               \n",
    "        feed_dict_batch = {x: xxx, y: xxx}                  #change xxx by your code\n",
    "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration % display_freq == 0:\n",
    "            # 3. Calculate and display the batch loss and accuracy\n",
    "            loss_batch, acc_batch = sess.run([xxx, xxx],              #change xxx by your code\n",
    "                                             feed_dict=feed_dict_batch)\n",
    "\n",
    "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                  format(iteration, loss_batch, acc_batch))\n",
    "    \n",
    "    # No action to be taken here, keep it as it is.\n",
    "    # Run validation after every epoch\n",
    "    feed_dict_valid = {x: x_valid[:1000].reshape((-1, timesteps, num_input)), y: y_valid[:1000]}\n",
    "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, loss_valid, acc_valid))\n",
    "    print('---------------------------------------------------------')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_WygEGqZwjo"
   },
   "source": [
    "# Generalization Performance over the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_EOSgEeZwjo"
   },
   "source": [
    "# Helper functions for Visualziations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYGRB1zHOj_z"
   },
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, title=None):\n",
    "    \"\"\"\n",
    "    Create figure with 3x3 sub-plots.\n",
    "    :param images: array of images to be plotted, (9, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels (9,)\n",
    "    :param cls_pred: corresponding true labels (9,)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(np.squeeze(images[i]).reshape(28, 28), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            ax_title = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            ax_title = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_title(ax_title)\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    if title:\n",
    "        plt.suptitle(title, size=20)\n",
    "    plt.show(block=False)\n",
    "\n",
    "def plot_example_errors(images, cls_true, cls_pred, title=None):\n",
    "    \"\"\"\n",
    "    Function for plotting examples of images that have been mis-classified\n",
    "    :param images: array of all images, (#imgs, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels, (#imgs,)\n",
    "    :param cls_pred: corresponding predicted labels, (#imgs,)\n",
    "    \"\"\"\n",
    "    # Negate the boolean array.\n",
    "    incorrect = np.logical_not(np.equal(cls_pred, cls_true))\n",
    "\n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    incorrect_images = images[incorrect]\n",
    "\n",
    "    # Get the true and predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "    cls_true = cls_true[incorrect]\n",
    "\n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=incorrect_images[0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9],\n",
    "                title=title)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjqQhkZsZwjo"
   },
   "source": [
    "### Computing the Model Performance over the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJyPxgOLOkCL"
   },
   "outputs": [],
   "source": [
    "# Test the network (only on 1000 samples) after training\n",
    "# Accuracy\n",
    "\n",
    "# Load the test set, use the test mode\n",
    "x_test, y_test = load_MNIST_data(xxx)   # Replace xxx by your code\n",
    "feed_dict_test = {x: x_test[:1000].reshape((-1, timesteps, num_input)), y: y_test[:1000]}\n",
    "\n",
    "# Compute the test loss and the accuracy:\n",
    "loss_test, acc_test = sess.run([xxx, xxxx], feed_dict=feed_dict_test)  # Replace xxx by your code\n",
    "print('---------------------------------------------------------')\n",
    "print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "\n",
    "# Plot some of the correct and misclassified examples\n",
    "cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n",
    "cls_true = np.argmax(y_test, axis=1)\n",
    "plot_images(x_test, cls_true, cls_pred, title='Correct Examples')\n",
    "plot_example_errors(x_test[:1000], cls_true[:1000], cls_pred, title='Misclassified Examples')\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbchdXLmZwjp"
   },
   "source": [
    "# Comment on the model performance, please consider the following aspects:\n",
    " 1. the running time to train the whole model\n",
    " 2. the performance in term of accuracy\n",
    " 3. the model complexity\n",
    " \n",
    " ---\n",
    " \n",
    " 4. Do you think using RNN is advantageous? Why? \n",
    " 5. If you are presented with a problem such as Digit Classification, would you directly consider using RNN? If No, what types of Neural Networks would you consider and why?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Recurrent_Neural_Network_Lab_Part_1_Q.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
